#!/usr/bin/env python3
import certifi
import requests
import os
from getpass import getpass
import pandas as pd
import io

# --- 1. SSL CERTIFICATE CONFIGURATION ---
cert_path = certifi.where()
os.environ["REQUESTS_CA_BUNDLE"] = cert_path
os.environ["SSL_CERT_FILE"] = cert_path

# --- 2. CREDENTIALS & CONFIG ---
username = input("Enter your Qualys username: ")
password = getpass("Enter your Qualys password: ")

BASE_URL = "https://qualysapi.qg4.apps.qualys.com"
ENDPOINT = "/qps/rest/2.0/search/am/hostasset"
api_endpoint = f"{BASE_URL}{ENDPOINT}"

HEADERS = {
    "X-Requested-With": "Python",
    "Content-Type": "text/xml",
    "Accept": "application/json"
}

TAG_NAME = 'Retail Store Prod Deployment 2025'
DAYS_SINCE_CHECKIN = 30  

all_hosts = []
last_id = 0
has_more = True

print(f"[*] Starting paginated fetch for tag: {TAG_NAME}...")

try:
    while has_more:
        # Use 'id' operator 'GREATER' to paginate through results
        # This is the most reliable way to get past the 100-record cap
        search_xml = f"""
        <ServiceRequest>
            <filters>
                <Criteria field="tagName" operator="EQUALS">{TAG_NAME}</Criteria>
                <Criteria field="id" operator="GREATER">{last_id}</Criteria>
            </filters>
            <responseOptions>
                <includeFields>agentInfo.lastCheckedIn,name,address,os,id</includeFields>
            </responseOptions>
        </ServiceRequest>
        """

        response = requests.post(
            api_endpoint,
            auth=(username, password),
            headers=HEADERS,
            data=search_xml,
            timeout=60
        )
        response.raise_for_status()
        data = response.json()

        batch = data.get('ServiceResponse', {}).get('data', [])
        
        if not batch:
            has_more = False
        else:
            all_hosts.extend(batch)
            # Update last_id to the ID of the last asset in this batch
            last_id = batch[-1].get('hostAsset', {}).get('id')
            print(f"[*] Retrieved {len(all_hosts)} assets so far...")
            
            # If we got fewer than 100, we've reached the end
            if len(batch) < 100:
                has_more = False

    if all_hosts:
        df = pd.json_normalize(all_hosts)
        print(f"[+] Total assets retrieved from API: {len(df)}")

        # --- 3. DYNAMIC COLUMN PICKER & FILTER ---
        date_col = next((c for c in df.columns if 'lastCheckedIn' in c), None)

        if date_col:
            df['lastCheckedIn_DT'] = pd.to_datetime(df[date_col]).dt.tz_localize(None)
            cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=DAYS_SINCE_CHECKIN)
            df_filtered = df[df['lastCheckedIn_DT'] >= cutoff_date].copy()

            output_file = 'qualys_assets_for_pbi.csv'
            df_filtered.to_csv(output_file, index=False)
            print(f"[+] Final count after {DAYS_SINCE_CHECKIN}-day filter: {len(df_filtered)}")
            print(f"[+] File saved: {output_file}")
    else:
        print("[!] No assets found for that tag.")

except Exception as e:
    print(f"[!] Error during pagination: {e}")
